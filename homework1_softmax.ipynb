{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"“homework1_softmax.ipynb”的副本","version":"0.3.2","provenance":[{"file_id":"1P4szZsGvPZNwZhtKWo1wXveup00Hdqk6","timestamp":1566836299664},{"file_id":"1Y6nCufBF4k9fe_DTflMvN3fNHId0Rmy9","timestamp":1535429950573}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qcUwZ2kYYV-q","colab_type":"text"},"source":["# Homework 1: Classifiers\n"]},{"cell_type":"markdown","metadata":{"id":"i2fakcGhJ08N","colab_type":"text"},"source":["# Linear Softmax Classifier\n","\n","This exercise is analogous to the SVM exercise. You will:\n","\n","- implement a fully-vectorized **loss function** for the Softmax classifier\n","- implement the fully-vectorized expression for its **analytic gradient**\n","- **check your implementation** with numerical gradient\n","- use a validation set to **tune the learning rate and regularization** strength\n","- **optimize** the loss function with **SGD**\n","- **visualize** the final learned weights\n"]},{"cell_type":"code","metadata":{"id":"Z3CGTpwFJ08P","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGrRij4Tqdxf","colab_type":"text"},"source":["## Load and preprocess CIFAR-10 dataset"]},{"cell_type":"code","metadata":{"id":"EpJ-YmcDJ08S","colab_type":"code","outputId":"c2574645-fb24-45c2-c61b-ec44329f95e9","executionInfo":{"status":"ok","timestamp":1567549851561,"user_tz":360,"elapsed":10185,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["from keras.datasets import cifar10\n","\n","def get_CIFAR10_data(num_training=49000, num_validation=1000,\n","                     num_test=1000, num_dev=500):\n","    \"\"\"\n","    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n","    it for the linear classifier. These are the same steps as we used for the\n","    SVM, but condensed to a single function.  \n","    \"\"\"\n","    # Load the raw CIFAR-10 data\n","    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n","    \n","    # All the data comes in the uint8 format, so we need to convert\n","    # it to floats so that we compute numbers greater than 255.\n","    X_train = X_train.astype(np.float)\n","    X_test = X_test.astype(np.float)\n","    # Also, for convenience we flatten the class arrays.\n","    y_train = y_train.flatten()\n","    y_test = y_test.flatten()\n","    \n","    # Split the data into train, val, and test sets. In addition we will\n","    # create a small development set as a subset of the training data;\n","    # we can use this for development so our code runs faster.\n","    \n","    # Our validation set will be num_validation points from the original\n","    # training set.\n","    mask = list(range(num_training, num_training + num_validation))\n","    X_val = X_train[mask]\n","    y_val = y_train[mask]\n","    \n","    # Our training set will be the first num_train points from the original\n","    # training set.\n","    mask = list(range(num_training))\n","    X_train = X_train[mask]\n","    y_train = y_train[mask]\n","    \n","    # We will also make a development set, which is a small subset of\n","    # the training set.\n","    mask = np.random.choice(num_training, num_dev, replace=False)\n","    X_dev = X_train[mask]\n","    y_dev = y_train[mask]\n","    \n","    # We use the first num_test points of the original test set as our\n","    # test set.\n","    mask = list(range(num_test))\n","    X_test = X_test[mask]\n","    y_test = y_test[mask]\n","    \n","    # Preprocessing: reshape the image data into rows\n","    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","    \n","    # Normalize the data: subtract the mean image\n","    mean_image = np.mean(X_train, axis = 0)\n","    X_train -= mean_image\n","    X_val -= mean_image\n","    X_test -= mean_image\n","    X_dev -= mean_image\n","    \n","    # third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n","    # only has to worry about optimizing a single weight matrix W.\n","    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","    \n","    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n","\n","\n","# Invoke the above function to get our data.\n","X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)\n","print('dev data shape: ', X_dev.shape)\n","print('dev labels shape: ', y_dev.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 2s 0us/step\n","Train data shape:  (49000, 3073)\n","Train labels shape:  (49000,)\n","Validation data shape:  (1000, 3073)\n","Validation labels shape:  (1000,)\n","Test data shape:  (1000, 3073)\n","Test labels shape:  (1000,)\n","dev data shape:  (500, 3073)\n","dev labels shape:  (500,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1YqRGKxjJ08V","colab_type":"text"},"source":["## Define a naive Softmax classifier loss function\n","\n","Next we define the Softmax loss function.  This will be a naive implementation using loops.  Most of the code for this loss function already exists, but you will need to write code of your own to finish it.  Follow the instructions in the TODO section.\n","\n","Recall that the contribution of a training point $(x_i, y_i)$ to the Softmax loss function is\n","\n","$$L_i = -\\log \\left( \\frac{\\exp(s_{y_i})}{\\sum_{j} \\exp(s_j)} \\right)$$\n","\n","This is the cross-entropy between the predicted class probabilities, and the distribution with all probability concentrated at $y_i$.  The score $s$ is again parametrized by a linear function $s_j = xW_j$ where $x$ is a single data sample and $W_j$ is the $j$th column of $W$."]},{"cell_type":"code","metadata":{"id":"HNzBpdmbY67R","colab_type":"code","colab":{}},"source":["def softmax_loss_naive(W, X, y, reg):\n","  \"\"\"\n","  Softmax loss function, naive implementation (with loops)\n","\n","  Inputs have dimension D, there are C classes, and we operate on minibatches\n","  of N examples.\n","\n","  Inputs:\n","  - W: A numpy array of shape (D, C) containing weights.\n","  - X: A numpy array of shape (N, D) containing a minibatch of data.\n","  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","    that X[i] has label c, where 0 <= c < C.\n","  - reg: (float) regularization strength\n","\n","  Returns a tuple of:\n","  - loss as single float\n","  - gradient with respect to weights W; an array of same shape as W\n","  \"\"\"\n","  # Initialize the loss and gradient to zero.\n","  loss = 0.0\n","  dW = np.zeros_like(W)\n","\n","\n","  #############################################################################\n","  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n","  # Store the loss in loss and the gradient in dW. If you are not careful     #\n","  # here, it is easy to run into numeric instability. Don't forget the        #\n","  # regularization!                                                           #\n","  #############################################################################\n","  N=X.shape[0]\n","  C=W.shape[1]\n","  l1_mul=X@W\n","  scores=l1_mul\n","  score_ex_sum=np.sum(np.exp(scores),1)[:,np.newaxis]\n","  y_onehot=np.zeros((N,C))\n","  y_onehot[np.arange(N),y]=1\n","  class_p=np.exp(scores)/score_ex_sum\n","\n","\n","  loss_class=np.sum(-y_onehot*np.log(class_p))/N\n","\n","  loss_reg=reg*(np.sum(W*W))\n","\n","  loss =loss_class+loss_reg\n","  dif_cls_tag=(class_p-y_onehot)\n","  for i in range(N):\n","      col=dif_cls_tag[i,:]\n","      row=X[i,:]\n","      dW=dW+row[:,np.newaxis]@col[np.newaxis,:]/N\n","  dW=dW+2*reg*W\n","\n","  #############################################################################\n","  #                          END OF YOUR CODE                                 #\n","  #############################################################################\n","\n","  return loss, dW"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJPp2yuBJ08W","colab_type":"code","outputId":"dd539094-deb5-40b8-f618-6a99ce5a5ac3","executionInfo":{"status":"ok","timestamp":1567549859913,"user_tz":360,"elapsed":580,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Evaluate the naive implementation of the loss we provided for you:\n","import time\n","\n","# Generate a random softmax weight matrix and use it to compute the loss.\n","W = np.random.randn(3073, 10) * 0.0001\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As a rough sanity check, our loss should be something close to -log(0.1).\n","print('loss: %f' % loss)\n","print('sanity check: %f' % (-np.log(0.1)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["loss: 2.279544\n","sanity check: 2.302585\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OSZOfaTpJ08Z","colab_type":"text"},"source":["## Inline Question 1:\n","Why do we expect our loss to be close to -log(0.1)? Explain briefly.\n","\n","**Your answer:** The initialized value for W is realtively small and roughly euqals to one another. Thus the scores  we get also roughly equal to one another. Thus the probabilities are roughly 0.1 for all categories. Thus, -ylog(p)=-log(0.1).\n"]},{"cell_type":"code","metadata":{"id":"vaIHaDRiZTP0","colab_type":"code","outputId":"c3f415f6-4e44-4be9-a4e3-edebf5eac4ac","executionInfo":{"status":"ok","timestamp":1567462801404,"user_tz":360,"elapsed":5086,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":384}},"source":["def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n","  \"\"\"\n","  sample a few random elements and only return numerical\n","  in this dimensions.\n","  \"\"\"\n","\n","  for i in range(num_checks):\n","    ix = tuple([np.random.randint(m) for m in x.shape])\n","\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evaluate f(x + h)\n","    x[ix] = oldval - h # increment by h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # reset\n","\n","    grad_numerical = (fxph - fxmh) / (2 * h)\n","    grad_analytic = analytic_grad[ix]\n","    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n","    print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n","    \n","# Complete the implementation of softmax_loss_naive and implement a (naive)\n","# version of the gradient that uses nested loops.\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n","\n","# As we did for the SVM, use numeric gradient checking as a debugging tool.\n","# The numeric gradient should be close to the analytic gradient.\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)\n","\n","# similar to SVM case, do another gradient check with regularization\n","loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n","f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad, 10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["numerical: 0.644668 analytic: 0.644668, relative error: 5.538157e-08\n","numerical: -1.223768 analytic: -1.223768, relative error: 4.116681e-08\n","numerical: 0.366189 analytic: 0.366189, relative error: 6.907518e-08\n","numerical: 0.993713 analytic: 0.993713, relative error: 2.791645e-08\n","numerical: 0.789464 analytic: 0.789464, relative error: 5.040894e-08\n","numerical: -2.671346 analytic: -2.671346, relative error: 6.465951e-09\n","numerical: 0.100656 analytic: 0.100656, relative error: 1.028403e-06\n","numerical: -1.553211 analytic: -1.553211, relative error: 1.523165e-08\n","numerical: -0.002174 analytic: -0.002174, relative error: 6.915738e-06\n","numerical: -0.218946 analytic: -0.218946, relative error: 4.688276e-07\n","numerical: 2.290310 analytic: 2.290310, relative error: 1.330151e-08\n","numerical: -1.864347 analytic: -1.864347, relative error: 2.524487e-08\n","numerical: 0.096345 analytic: 0.096345, relative error: 3.135227e-08\n","numerical: 0.866156 analytic: 0.866156, relative error: 7.877320e-08\n","numerical: -0.147870 analytic: -0.147870, relative error: 4.931091e-07\n","numerical: 2.644697 analytic: 2.644696, relative error: 8.181434e-09\n","numerical: -2.771061 analytic: -2.771061, relative error: 5.320693e-08\n","numerical: 1.328758 analytic: 1.328758, relative error: 9.959017e-09\n","numerical: -1.468256 analytic: -1.468256, relative error: 1.222025e-08\n","numerical: -1.530169 analytic: -1.530169, relative error: 4.326364e-08\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AW-ddDOCsXMW","colab_type":"text"},"source":["## Define a vectorized Softmax classifier loss function\n","\n","Next we define the vectorized (i.e. no loops) version of the Softmax loss function.  Most of the code for this loss function already exists, but you will need to write code of your own to finish it.  Follow the instructions in the TODO section."]},{"cell_type":"code","metadata":{"id":"D2HDgzaMZfRV","colab_type":"code","colab":{}},"source":["def softmax_loss_vectorized(W, X, y, reg):\n","  \"\"\"\n","  Softmax loss function, vectorized version.\n","\n","  Inputs and outputs are the same as softmax_loss_naive.\n","  \"\"\"\n","  # Initialize the loss and gradient to zero.\n","  loss = 0.0\n","  dW = np.zeros_like(W)\n","\n","  #############################################################################\n","  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n","  # Store the loss in loss and the gradient in dW. If you are not careful     #\n","  # here, it is easy to run into numeric instability. Don't forget the        #\n","  # regularization!                                                           #\n","  #############################################################################\n","  N,D=X.shape\n","  C=W.shape[1]\n","  \n","  l1_mul=X@W\n","  scores=l1_mul\n","  score_ex_sum=np.sum(np.exp(scores),1)[:,np.newaxis]\n","  y_onehot=np.zeros((N,C))\n","  y_onehot[np.arange(N),y]=1\n","  class_p=np.exp(scores)/score_ex_sum\n","    #dif_cls_tagdim=np.repeat(dif_cls_tag[:,np.newaxis,:],H,1)\n","    #l1_reludim=np.repeat(l1_relu[:,:,np.newaxis],C,2)\n","    #W2_gradient=np.sum(dif_cls_tagdim*l1_reludim,0)\n","\n","  loss_class=np.sum(-y_onehot*np.log(class_p))/N\n","\n","  loss_reg=reg*(np.sum(W*W))\n","\n","  loss =loss_class+loss_reg\n","  dif_cls_tag=(class_p-y_onehot)\n","  #dif_cls_tagdim=np.repeat(dif_cls_tag[:,np.newaxis,:],D,1)\n","  #l1_reludim=np.repeat(X[:,:,np.newaxis],C,2)\n","  #dW=np.sum(dif_cls_tagdim*l1_reludim,0)/N\n","  dW=X.T@dif_cls_tag/N\n","  dW=dW+2*reg*W\n","  #############################################################################\n","  #                          END OF YOUR CODE                                 #\n","  #############################################################################\n","\n","  return loss, dW"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSxdTSBPJ08d","colab_type":"code","outputId":"baf7d2d1-61ed-43f5-f570-69c81fe6d2fe","executionInfo":{"status":"ok","timestamp":1567549908883,"user_tz":360,"elapsed":415,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["# Evaluate the naive implementation of the Softmax gradients\n","\n","# The naive implementation and the vectorized implementation should match, but\n","# the vectorized version should still be much faster.\n","tic = time.time()\n","loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n","\n","tic = time.time()\n","loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n","toc = time.time()\n","print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n","\n","# As we did for the SVM, we use the Frobenius norm to compare the two versions\n","# of the gradient.\n","grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n","print('Gradient difference: %f' % grad_difference)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["naive loss: 2.279544e+00 computed in 0.111749s\n","vectorized loss: 2.279544e+00 computed in 0.009889s\n","Loss difference: 0.000000\n","Gradient difference: 0.000000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c6zzNjOgtYDq","colab_type":"text"},"source":["## Stochastic Gradient Descent\n","\n","We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss. Follow the instructions in the TODO sections below.  You may just want to copy the code you wrote for the SVM."]},{"cell_type":"code","metadata":{"id":"FRZYRkF7ZzE0","colab_type":"code","colab":{}},"source":["class Softmax(object):\n","\n","  def __init__(self):\n","    self.W = None\n","\n","  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n","            batch_size=200, verbose=False):\n","    \"\"\"\n","    Train this linear classifier using stochastic gradient descent.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) containing training data; there are N\n","      training samples each of dimension D.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n","      means that X[i] has label 0 <= c < C for C classes.\n","    - learning_rate: (float) learning rate for optimization.\n","    - reg: (float) regularization strength.\n","    - num_iters: (integer) number of steps to take when optimizing\n","    - batch_size: (integer) number of training examples to use at each step.\n","    - verbose: (boolean) If true, print progress during optimization.\n","\n","    Outputs:\n","    A list containing the value of the loss function at each training iteration.\n","    \"\"\"\n","    num_train, dim = X.shape\n","    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","    if self.W is None:\n","      # lazily initialize W\n","      self.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    for it in range(num_iters):\n","      X_batch = None\n","      y_batch = None\n","\n","      #########################################################################\n","      # TODO:                                                                 #\n","      # Sample batch_size elements from the training data and their           #\n","      # corresponding labels to use in this round of gradient descent.        #\n","      # Store the data in X_batch and their corresponding labels in           #\n","      # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n","      # and y_batch should have shape (batch_size,)                           #\n","      #                                                                       #\n","      # Hint: Use np.random.choice to generate indices. Sampling with         #\n","      # replacement is faster than sampling without replacement.              #\n","      #########################################################################\n","      batch_ind=np.random.choice(num_train, batch_size)\n","      X_batch=X[batch_ind,:]\n","      y_batch=y[batch_ind]\n","      #########################################################################\n","      #                       END OF YOUR CODE                                #\n","      #########################################################################\n","\n","      # evaluate loss and gradient\n","      loss, grad = self.loss(X_batch, y_batch, reg)\n","      loss_history.append(loss)\n","\n","      # perform parameter update\n","      #########################################################################\n","      # TODO:                                                                 #\n","      # Update the weights using the gradient and the learning rate.          #\n","      #########################################################################\n","      self.W=self.W-grad*learning_rate\n","      #########################################################################\n","      #                       END OF YOUR CODE                                #\n","      #########################################################################\n","\n","      if verbose and it % 100 == 0:\n","        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","    return loss_history\n","\n","  def predict(self, X):\n","    \"\"\"\n","    Use the trained weights of this linear classifier to predict labels for\n","    data points.\n","\n","    Inputs:\n","    - X: A numpy array of shape (N, D) containing training data; there are N\n","      training samples each of dimension D.\n","\n","    Returns:\n","    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","      array of length N, and each element is an integer giving the predicted\n","      class.\n","    \"\"\"\n","    y_pred = np.zeros(X.shape[0])\n","    ###########################################################################\n","    # TODO:                                                                   #\n","    # Implement this method. Store the predicted labels in y_pred.            #\n","    ###########################################################################\n","    y_pred =np.argmax(X@self.W,1)\n","    ###########################################################################\n","    #                           END OF YOUR CODE                              #\n","    ###########################################################################\n","    return y_pred\n","  \n","  def loss(self, X_batch, y_batch, reg):\n","    \"\"\"\n","    Compute the loss function and its derivative. \n","    Subclasses will override this.\n","\n","    Inputs:\n","    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n","      data points; each point has dimension D.\n","    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n","    - reg: (float) regularization strength.\n","\n","    Returns: A tuple containing:\n","    - loss as a single float\n","    - gradient with respect to self.W; an array of the same shape as W\n","    \"\"\"\n","    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOKw3bKKJ08f","colab_type":"code","outputId":"bbd89f6b-7119-48a3-c3d2-8df64f031296","executionInfo":{"status":"ok","timestamp":1567477924251,"user_tz":360,"elapsed":747139,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths; if you are careful you should be able to\n","# get a classification accuracy of over 0.35 on the validation set.\n","results = {}\n","best_val = -1\n","best_softmax = None\n","\n","learning_rates = [1e-7, 5e-7]\n","regularization_strengths = [2.5e4, 5e4]\n","################################################################################\n","# TODO:                                                                        #\n","# Use the validation set to set the learning rate and regularization strength. #\n","# This should be identical to the validation that you did for the SVM; save    #\n","# the best trained softmax classifer in best_softmax.                          #\n","################################################################################\n","def param_tuning(learning_rates,regularization_strengths,results):\n","    best_val = -1\n","    best_softmax = None\n","    shape_coarse=5\n","    lr=np.random.uniform(learning_rates[0],learning_rates[1],(shape_coarse))\n","    reg=np.random.uniform(regularization_strengths[0],regularization_strengths [1],(shape_coarse))\n","    for i in range(shape_coarse):\n","      soft_coarse=Softmax()\n","      soft_coarse.train(X_train, y_train, learning_rate=lr[i], reg=reg[i], num_iters=500)\n","      soft_coarse.predict(X_val)\n","      train_acc = (soft_coarse.predict(X_train) == y_train).mean()\n","      val_acc = (soft_coarse.predict(X_val) == y_val).mean()\n","      param=(lr[i],reg[i])\n","      acc=(train_acc,val_acc)\n","      results[param]=acc\n","      print('lr %e reg %e train accuracy: %f val accuracy: %f' % (lr[i], reg[i], train_acc, val_acc))\n","      if val_acc>best_val:\n","        best_val=val_acc\n","        best_softmax=soft_coarse\n","    results_key=sorted(results,key=results.get)\n","    tuned_lr=[0.1,0.1]\n","    tuned_reg=[0.1,0.1]\n","    tuned_lr[0],tuned_reg[0]=results_key[-1]\n","    tuned_lr[1],tuned_reg[1]=results_key[-2]\n","      \n","    return  tuned_lr,tuned_reg,best_softmax,results,best_val\n","learning_rates,regularization_strengths,best_softmax,results,best_val=param_tuning(learning_rates,regularization_strengths,results)\n","learning_rates,regularization_strengths,best_softmax,results,best_val=param_tuning(learning_rates,regularization_strengths,results)\n","learning_rates,regularization_strengths,best_softmax,results,best_val=param_tuning(learning_rates,regularization_strengths,results)\n","\n","################################################################################\n","#                              END OF YOUR CODE                                #\n","################################################################################\n","    \n","# Print out results.\n","for lr, reg in sorted(results):\n","    train_accuracy, val_accuracy = results[(lr, reg)]\n","    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n","                lr, reg, train_accuracy, val_accuracy))\n","    \n","print('best validation accuracy achieved during cross-validation: %f' % best_val)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["lr 1.175209e-07 reg 3.340997e+04 train accuracy: 0.318041 val accuracy: 0.329000\n","lr 2.670499e-07 reg 3.926067e+04 train accuracy: 0.312980 val accuracy: 0.324000\n","lr 2.995830e-07 reg 4.538325e+04 train accuracy: 0.309306 val accuracy: 0.323000\n","lr 3.409487e-07 reg 3.875751e+04 train accuracy: 0.314102 val accuracy: 0.326000\n","lr 3.907062e-07 reg 2.518027e+04 train accuracy: 0.332061 val accuracy: 0.354000\n","lr 3.174203e-07 reg 3.141075e+04 train accuracy: 0.322082 val accuracy: 0.335000\n","lr 2.501777e-07 reg 3.171378e+04 train accuracy: 0.324939 val accuracy: 0.340000\n","lr 1.887750e-07 reg 3.205459e+04 train accuracy: 0.310980 val accuracy: 0.327000\n","lr 1.745349e-07 reg 2.655459e+04 train accuracy: 0.326755 val accuracy: 0.347000\n","lr 2.263997e-07 reg 3.080240e+04 train accuracy: 0.318939 val accuracy: 0.329000\n","lr 2.943874e-07 reg 2.542408e+04 train accuracy: 0.324082 val accuracy: 0.340000\n","lr 2.687707e-07 reg 2.549900e+04 train accuracy: 0.326816 val accuracy: 0.344000\n","lr 2.935595e-07 reg 2.587000e+04 train accuracy: 0.329878 val accuracy: 0.346000\n","lr 1.761146e-07 reg 2.520954e+04 train accuracy: 0.326796 val accuracy: 0.351000\n","lr 2.119089e-07 reg 2.525073e+04 train accuracy: 0.326122 val accuracy: 0.337000\n","lr 1.175209e-07 reg 3.340997e+04 train accuracy: 0.318041 val accuracy: 0.329000\n","lr 1.745349e-07 reg 2.655459e+04 train accuracy: 0.326755 val accuracy: 0.347000\n","lr 1.761146e-07 reg 2.520954e+04 train accuracy: 0.326796 val accuracy: 0.351000\n","lr 1.887750e-07 reg 3.205459e+04 train accuracy: 0.310980 val accuracy: 0.327000\n","lr 2.119089e-07 reg 2.525073e+04 train accuracy: 0.326122 val accuracy: 0.337000\n","lr 2.263997e-07 reg 3.080240e+04 train accuracy: 0.318939 val accuracy: 0.329000\n","lr 2.501777e-07 reg 3.171378e+04 train accuracy: 0.324939 val accuracy: 0.340000\n","lr 2.670499e-07 reg 3.926067e+04 train accuracy: 0.312980 val accuracy: 0.324000\n","lr 2.687707e-07 reg 2.549900e+04 train accuracy: 0.326816 val accuracy: 0.344000\n","lr 2.935595e-07 reg 2.587000e+04 train accuracy: 0.329878 val accuracy: 0.346000\n","lr 2.943874e-07 reg 2.542408e+04 train accuracy: 0.324082 val accuracy: 0.340000\n","lr 2.995830e-07 reg 4.538325e+04 train accuracy: 0.309306 val accuracy: 0.323000\n","lr 3.174203e-07 reg 3.141075e+04 train accuracy: 0.322082 val accuracy: 0.335000\n","lr 3.409487e-07 reg 3.875751e+04 train accuracy: 0.314102 val accuracy: 0.326000\n","lr 3.907062e-07 reg 2.518027e+04 train accuracy: 0.332061 val accuracy: 0.354000\n","best validation accuracy achieved during cross-validation: 0.351000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rY18wElRJ08i","colab_type":"code","outputId":"a88c1dfd-6777-496e-ebe7-b27af4dc3396","executionInfo":{"status":"ok","timestamp":1567472429167,"user_tz":360,"elapsed":399,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# evaluate on test set\n","# Evaluate the best softmax on test set\n","y_test_pred = best_softmax.predict(X_test)\n","test_accuracy = np.mean(y_test == y_test_pred)\n","print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["softmax on raw pixels final test set accuracy: 0.346000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-iwERfXwJ08l","colab_type":"code","outputId":"fa785367-b31c-4655-e5bf-0b0fb1f0afb6","executionInfo":{"status":"ok","timestamp":1567472446646,"user_tz":360,"elapsed":960,"user":{"displayName":"Tao Yang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAl_-44hB9cBW2Ohbv7EbdHeVSG-wL9nJFb0tIp=s64","userId":"14425838613441090609"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"source":["# Visualize the learned weights for each class\n","w = best_softmax.W[:-1,:] # strip out the bias\n","w = w.reshape(32, 32, 3, 10)\n","\n","w_min, w_max = np.min(w), np.max(w)\n","\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAADiCAYAAABeKzy5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHR0dL4K+oXd0dHS8CP4fpoOTn/msWZoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"7EFiDVPKejJy","colab_type":"text"},"source":["*Many thanks to Stanford CS231n for permission to use their materials!*"]}]}